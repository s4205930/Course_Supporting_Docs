{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc440994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, precision_score, recall_score,\n",
    "    hamming_loss, accuracy_score\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import timm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import csv\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35486a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv_dir, image_dir, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        df = pd.read_csv(csv_dir)\n",
    "\n",
    "        # Extract labels and image names\n",
    "        self.labels = df.iloc[:, -14:].values.astype(\"float32\")\n",
    "        self.image_names = df['Path'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_names[idx])\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')  # RGB\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image: {img_path} - {e}\")\n",
    "            raise e\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "\n",
    "        image = np.array(image)\n",
    "\n",
    "        label = np.array(self.labels[idx])\n",
    "        \n",
    "        return {'images': image, 'labels': label}\n",
    "\n",
    "def compute_sample_weights(labels):\n",
    "    \"\"\"\n",
    "    Computes sample weights for multi-label oversampling.\n",
    "    labels: numpy array of shape (N, num_classes)\n",
    "    \"\"\"\n",
    "    label_counts = np.sum(labels, axis=0)  # count per class\n",
    "    class_weights = 1.0 / (label_counts + 1e-6)  # avoid division by zero\n",
    "\n",
    "    # Sample weights per image: mean of the weights of the labels present\n",
    "    sample_weights = np.dot(labels, class_weights)\n",
    "    return sample_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 224\n",
    "size = (dim, dim)\n",
    "\n",
    "trans_train = transforms.Compose([\n",
    "    transforms.Resize(size),                      # Resize first\n",
    "    transforms.RandomHorizontalFlip(p=0.5),      # 50% chance flip\n",
    "    transforms.RandomRotation(15),                # Â±15 degrees rotation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # brightness/contrast jitter\n",
    "    transforms.RandomResizedCrop(size, scale=(0.8, 1.0)),  # random zoom/crop\n",
    "    transforms.RandomPerspective(distortion_scale=0.1, p=0.5), # optional perspective distortion\n",
    "    transforms.Grayscale(num_output_channels=3), # convert to 3 channel grayscale\n",
    "    transforms.ToTensor(),                         # convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet normalisation standard\n",
    "                        std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "trans_val = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "ds_size = \"Balanced\"\n",
    "\n",
    "train_data = ImageDataset(fr\"NIH_X_14_{ds_size}\\train_labels_encoded.csv\", fr\"NIH_X_14_{ds_size}\\train\", transforms=trans_train)\n",
    "test_data = ImageDataset(fr\"NIH_X_14_{ds_size}\\test_labels_encoded.csv\", fr\"NIH_X_14_{ds_size}\\test\", transforms=trans_val)\n",
    "val_data = ImageDataset(fr\"NIH_X_14_{ds_size}\\val_labels_encoded.csv\", fr\"NIH_X_14_{ds_size}\\val\", transforms=trans_val)\n",
    "\n",
    "\n",
    "class_names = [\n",
    "    'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema',\n",
    "    'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration',\n",
    "    'Mass', 'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62464462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute sample weights from label frequency ---\n",
    "label_array = np.array(train_data.labels)  # Shape: (N_samples, N_classes)\n",
    "class_counts = np.sum(label_array, axis=0)  # total positives per class\n",
    "class_weights = 1.0 / (class_counts + 1e-6)  # inverse frequency\n",
    "class_weights = class_weights / np.sum(class_weights) * len(class_weights)  # normalize\n",
    "\n",
    "print(\"Class counts:\", class_counts)\n",
    "print(\"Class weights (normalized):\", np.round(class_weights, 4))\n",
    "\n",
    "sample_weights = np.dot(label_array, class_weights)  # (N_samples,)\n",
    "sample_weights = sample_weights / np.sum(sample_weights) * len(sample_weights)\n",
    "sample_weights = np.clip(sample_weights, a_min=1e-6, a_max=None)\n",
    "\n",
    "print(f\"Sample weights: mean={sample_weights.mean():.4e}, std={sample_weights.std():.4e}, max={sample_weights.max():.4e}\")\n",
    "\n",
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "if imbalance_ratio > 10:\n",
    "    print(f\"Warning: Significant class imbalance detected (max/min = {imbalance_ratio:.2f}). Consider data augmentation or other sampling strategies.\")\n",
    "\n",
    "sample_weights_tensor = torch.DoubleTensor(sample_weights)\n",
    "sampler = WeightedRandomSampler(sample_weights_tensor, len(sample_weights_tensor), replacement=True)\n",
    "\n",
    "# Sanity check\n",
    "unique_indices = set([i for i in sampler])\n",
    "if len(unique_indices) < 0.5 * len(train_data):\n",
    "    print(f\"Sampler is using only {len(unique_indices)} unique samples out of {len(train_data)}. Check weights.\")\n",
    "\n",
    "# --- DataLoaders ---\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size, sampler=sampler, drop_last=True, pin_memory=True)\n",
    "test_dl = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True, pin_memory=True)\n",
    "val_dl = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d40a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ViT Model ---\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, model_name, num_classes=14, pretrained=True, in_chans=3):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes, in_chans=in_chans)\n",
    "        self.model_name = model_name\n",
    "        self.epochs_trained = 0\n",
    "\n",
    "        # Adjust classifier if necessary\n",
    "        self.model.reset_classifier(num_classes)\n",
    "\n",
    "        # Freeze all parameters first\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze classifier head only\n",
    "        if hasattr(self.model, 'head'):\n",
    "            for param in self.model.head.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # Unfreeze layer norm if present (often after blocks)\n",
    "        if hasattr(self.model, 'norm'):\n",
    "            for param in self.model.norm.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def fine_tune(self):\n",
    "        # Unfreeze all model parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1afa256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Focal Loss ---\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        targets = targets.type(inputs.dtype)\n",
    "\n",
    "        ce_loss = -(\n",
    "            self.alpha * targets * torch.log(probs + 1e-8) +\n",
    "            (1 - self.alpha) * (1 - targets) * torch.log(1 - probs + 1e-8)\n",
    "        )\n",
    "        focal_loss = (1 - probs) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "        \n",
    "# --- Asymmetric Loss ---\n",
    "class AsymmetricLoss(nn.Module):\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        preds = torch.sigmoid(preds)\n",
    "        preds = preds.clamp(self.eps, 1. - self.eps)\n",
    "\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            preds = (preds - self.clip).clamp(min=0)\n",
    "\n",
    "        pos_loss = targets * torch.log(preds) * ((1 - preds) ** self.gamma_pos)\n",
    "        neg_loss = (1 - targets) * torch.log(1 - preds) * (preds ** self.gamma_neg)\n",
    "\n",
    "        return - (pos_loss + neg_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c7fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d21b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_models = [\n",
    "    \"vit_tiny_patch16_224\",        # Lightweight, fast\n",
    "    \"vit_small_patch16_224\",       # Small and efficient\n",
    "    \"vit_base_patch16_224\",        # Standard ViT baseline\n",
    "    \"vit_large_patch16_224\",       # More capacity, slower\n",
    "\n",
    "    \"deit_tiny_patch16_224\",       # Data-efficient Image Transformer (DEiT) - tiny\n",
    "    \"deit_small_patch16_224\",      # DEiT - small\n",
    "    \"deit_base_patch16_224\",       # DEiT - standard\n",
    "\n",
    "    \"deit3_small_patch16_224\",     # DEiT v3 - improved training, stronger baseline\n",
    "    \"deit3_medium_patch16_224\",    # Balanced performance\n",
    "    \"deit3_base_patch16_224\",      # DEiT v3 - base\n",
    "    \"deit3_large_patch16_224\",     # DEiT v3 - large (resource heavy)\n",
    "\n",
    "    \"vit_relpos_base_patch16_224\", # Relative positional encoding\n",
    "    \"vit_relpos_small_patch16_224\",# Small version with relative pos\n",
    "\n",
    "    \"swin_tiny_patch4_window7_224\",# Swin Transformer (shifted windows)\n",
    "    \"swin_small_patch4_window7_224\",\n",
    "    \"swin_base_patch4_window7_224\",\n",
    "\n",
    "    \"beit_base_patch16_224\",       # BEiT (BERT-style masked image pretraining)\n",
    "    \"beit_large_patch16_224\"       # Larger BEiT\n",
    "]\n",
    "\n",
    "\n",
    "# --- Model Setup ---\n",
    "model = ViT(model_name='swin_base_patch4_window7_224', num_classes=14)\n",
    "model.fine_tune()\n",
    "model = model.to(device)\n",
    "\n",
    "eta = 1e-4\n",
    "opti = torch.optim.AdamW(model.parameters(), lr=eta, weight_decay=0.01)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb20951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Learning Rate Scheduler ---\n",
    "epochs = 150\n",
    "warmup_steps = len(train_dl) * 2\n",
    "total_steps = len(train_dl) * epochs\n",
    "scheduler = get_cosine_schedule_with_warmup(opti, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# --- Metrics Tracking ---\n",
    "epoch_train_loss, epoch_val_loss = [], []\n",
    "val_roc_aucs_by_epoch, val_roc_auc_means = [], []\n",
    "hammings, precisions, recalls, f1_macros, f1_micros = [], [], [], [], []\n",
    "class_thresholds = np.full(14, 0.35)\n",
    "best_auc = 0.0\n",
    "early_stop_counter = 0\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ced18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_log = []\n",
    "metrics_csv_path = f\"Metrics/{model.model_name}_training_metrics.csv\"\n",
    "os.makedirs(os.path.dirname(metrics_csv_path), exist_ok=True)\n",
    "os.makedirs(\"Checkpoints\", exist_ok=True)  # Ensure checkpoint folder exists\n",
    "\n",
    "roc_auc_columns = [f\"roc_auc_{name}\" for name in class_names]\n",
    "\n",
    "# Write header if file doesn't exist\n",
    "if not os.path.exists(metrics_csv_path):\n",
    "    with open(metrics_csv_path, mode='w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"epoch\", \"train_loss\", \"val_loss\", \"mean_auc\", \"mean_entropy\",\n",
    "            \"f1_micro\", \"f1_macro\", \"precision\", \"recall\", \"hamming_loss\"\n",
    "        ] + roc_auc_columns)\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    model.train()\n",
    "\n",
    "    for d in tqdm(train_dl, desc=f\"Training {epoch+1}/{epochs}\"):\n",
    "        opti.zero_grad()\n",
    "        imgs = d['images'].to(device, dtype=torch.float)\n",
    "        labels = d['labels'].to(device, dtype=torch.float)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        opti.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    epoch_train_loss.append(np.mean(train_losses))\n",
    "    model.epochs_trained += 1\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_losses, val_preds, val_targets = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(val_dl, desc=\"Validating\"):\n",
    "            imgs = d['images'].to(device, dtype=torch.float)\n",
    "            labels = d['labels'].to(device, dtype=torch.float)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "            preds = torch.sigmoid(logits)\n",
    "            val_preds.append(preds.cpu())\n",
    "            val_targets.append(labels.cpu())\n",
    "\n",
    "    epoch_val_loss.append(np.mean(val_losses))\n",
    "    all_val_preds = torch.cat(val_preds).numpy()\n",
    "    all_val_targets = torch.cat(val_targets).numpy()\n",
    "\n",
    "    entropy = - (all_val_preds * np.log(all_val_preds + 1e-7) + (1 - all_val_preds) * np.log(1 - all_val_preds + 1e-7))\n",
    "    mean_entropy = np.mean(entropy)\n",
    "\n",
    "    val_epoch_roc_aucs = []\n",
    "    for i in range(all_val_targets.shape[1]):\n",
    "        try:\n",
    "            auc = roc_auc_score(all_val_targets[:, i], all_val_preds[:, i])\n",
    "        except ValueError:\n",
    "            auc = float('nan')\n",
    "        val_epoch_roc_aucs.append(auc)\n",
    "\n",
    "    val_roc_aucs_by_epoch.append(val_epoch_roc_aucs)\n",
    "    mean_auc = np.nanmean(val_epoch_roc_aucs)\n",
    "    val_roc_auc_means.append(mean_auc)\n",
    "\n",
    "    # --- Thresholding ---\n",
    "    binary_preds = np.zeros_like(all_val_preds)\n",
    "    for i, thresh in enumerate(class_thresholds):\n",
    "        binary_preds[:, i] = (all_val_preds[:, i] > thresh).astype(int)\n",
    "\n",
    "    f1_micro = f1_score(all_val_targets, binary_preds, average='micro', zero_division=0)\n",
    "    f1_macro = f1_score(all_val_targets, binary_preds, average='macro', zero_division=0)\n",
    "    precision = precision_score(all_val_targets, binary_preds, average='micro', zero_division=0)\n",
    "    recall = recall_score(all_val_targets, binary_preds, average='micro', zero_division=0)\n",
    "    hamming = hamming_loss(all_val_targets, binary_preds)\n",
    "    label_accuracy = (binary_preds == all_val_targets).mean(axis=0)\n",
    "\n",
    "    f1_micros.append(f1_micro)\n",
    "    f1_macros.append(f1_macro)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    hammings.append(hamming)\n",
    "\n",
    "    # --- Epoch Summary ---\n",
    "    print(f\"Train Loss: {epoch_train_loss[-1]:.4f} | Val Loss: {epoch_val_loss[-1]:.4f}\")\n",
    "    print(f\"Mean ROC-AUC: {mean_auc:.4f} | Mean Entropy: {mean_entropy:.4f}\")\n",
    "    print(f\"F1-score (micro): {f1_micro:.4f} | F1-score (macro): {f1_macro:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "    print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "    print(f\"Label-wise Accuracy: {label_accuracy.round(3)}\\n\")\n",
    "\n",
    "    # Save metrics to CSV using model.epochs_trained\n",
    "    with open(metrics_csv_path, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            model.epochs_trained,\n",
    "            epoch_train_loss[-1],\n",
    "            epoch_val_loss[-1],\n",
    "            mean_auc,\n",
    "            mean_entropy,\n",
    "            f1_micro,\n",
    "            f1_macro,\n",
    "            precision,\n",
    "            recall,\n",
    "            hamming\n",
    "        ] + val_epoch_roc_aucs)\n",
    "\n",
    "    # --- Checkpoint & Early Stopping ---\n",
    "    if mean_auc > best_auc:\n",
    "        best_auc = mean_auc\n",
    "        early_stop_counter = 0\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimiser_state_dict': opti.state_dict(),\n",
    "            'epochs_trained': model.epochs_trained,\n",
    "        }, f\"Checkpoints/{model.model_name}_best.pth\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0afdb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_train_loss, label='Train Loss')\n",
    "plt.plot(epoch_val_loss, label='Validation Loss')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(f1_micros, label='F1 Micros')\n",
    "plt.plot(f1_macros, label='F1 Macros')\n",
    "plt.title(\"F1 Scores\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(precisions, label='Precision')\n",
    "plt.plot(recalls, label='Recall')\n",
    "plt.title(\"P&R Rates\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hammings, label='Hamming Loss')\n",
    "plt.title(\"Hamming Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "val_roc_aucs_by_epoch_np = np.array(val_roc_aucs_by_epoch)\n",
    "epochs_range = range(0, len(val_roc_aucs_by_epoch_np))\n",
    "\n",
    "# Per-class curves\n",
    "try:\n",
    "    for i in range(val_roc_aucs_by_epoch_np.shape[1]):\n",
    "        plt.plot(epochs_range, val_roc_aucs_by_epoch_np[:, i], label=class_names[i], alpha=0.5)\n",
    "\n",
    "    # Mean ROC-AUC\n",
    "    plt.plot(epochs_range, val_roc_auc_means, label='Mean ROC-AUC', color='black', linewidth=2.5)\n",
    "\n",
    "    plt.title(\"Validation ROC-AUC per Class and Mean\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"ROC-AUC\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except:\n",
    "    print(\"Empty variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9314c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "all_probs = []\n",
    "all_logits = []\n",
    "all_targets = []\n",
    "all_images = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for d in tqdm(test_dl, desc=\"Generating predictions\"):\n",
    "        imgs = d['images'].to(device, dtype=torch.float)\n",
    "        labels = d['labels'].to(device, dtype=torch.float)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        prob = torch.sigmoid(logits)\n",
    "\n",
    "        all_logits.append(logits.cpu())\n",
    "        all_probs.append(prob.cpu())\n",
    "        all_targets.append(labels.cpu())\n",
    "        all_images.append(imgs.cpu())\n",
    "\n",
    "# Stack into single arrays\n",
    "all_probs = torch.cat(all_probs).numpy()\n",
    "all_targets = torch.cat(all_targets).numpy()\n",
    "all_logits = torch.cat(all_logits).numpy()\n",
    "all_images = torch.cat(all_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20294f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresholds = []\n",
    "\n",
    "for i in range(all_targets.shape[1]):\n",
    "    scores = []\n",
    "    for t in tqdm(np.arange(0, 1, 0.01), desc=f\"Optimising threshold of class {i+1}\"):\n",
    "        probs = (all_probs[:, i] > t).astype(int)\n",
    "        f1 = f1_score(all_targets[:, i], probs, zero_division=0)\n",
    "        scores.append((t, f1))\n",
    "    best_threshold = max(scores, key=lambda x: x[1])[0]\n",
    "    best_thresholds.append(best_threshold)\n",
    "\n",
    "print(\"Best thresholds:\", best_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dbbb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = np.array([\n",
    "    (all_probs[:, i] > best_thresholds[i]).astype(int)\n",
    "    for i in range(all_probs.shape[1])\n",
    "]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1948cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_aucs = []\n",
    "\n",
    "for i in range(all_targets.shape[1]):\n",
    "    try:\n",
    "        auc = roc_auc_score(all_targets[:, i], all_preds[:, i])\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    roc_aucs.append(auc)\n",
    "\n",
    "print(\"\\nPer-Class ROC-AUC:\")\n",
    "for i, auc in enumerate(roc_aucs):\n",
    "    print(f\"Class {i}: {auc:.4f}\")\n",
    "\n",
    "print(f\"\\nMacro ROC-AUC: {np.nanmean(roc_aucs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "opti_preds = np.zeros_like(all_preds)\n",
    "\n",
    "for i in range(all_preds.shape[1]):\n",
    "    opti_preds[:, i] = (all_preds[:, i] > best_thresholds[i]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction(img_tensor, true_labels, pred_labels, opti_labels, class_names):\n",
    "    # Convert image to numpy array if it's a tensor\n",
    "    if isinstance(img_tensor, torch.Tensor):\n",
    "        img = img_tensor.cpu().numpy()\n",
    "    else:\n",
    "        img = np.array(img_tensor)\n",
    "\n",
    "    # Transpose if image has 3 channels (C, H, W) -> (H, W, C)\n",
    "    if img.ndim == 3 and img.shape[0] == 3:\n",
    "        img = np.transpose(img, (1, 2, 0))  # (C, H, W) -> (H, W, C)\n",
    "\n",
    "    # Clip values to [0, 1] in case they exceed due to normalisation\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    # Convert labels to numpy if they're tensors\n",
    "    if isinstance(true_labels, torch.Tensor):\n",
    "        true_labels = true_labels.cpu().numpy()\n",
    "    if isinstance(pred_labels, torch.Tensor):\n",
    "        pred_labels = pred_labels.cpu().numpy()\n",
    "    if isinstance(opti_labels, torch.Tensor):\n",
    "        opti_labels = opti_labels.cpu().numpy()\n",
    "\n",
    "    # Get active labels\n",
    "    true_txt = \", \".join([class_names[i] for i, v in enumerate(true_labels) if v == 1])\n",
    "    pred_txt = \", \".join([class_names[i] for i, v in enumerate(pred_labels) if v == 1])\n",
    "    opti_pred_txt = \", \".join([class_names[i] for i, v in enumerate(opti_labels) if v == 1])\n",
    "\n",
    "    # Plotting\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"True: {true_txt}\\nPred(std): {pred_txt}\\nPred(Opti): {opti_pred_txt}\", fontsize=10)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "idx = random.randint(0, len(all_images)-1)\n",
    "show_prediction(all_images[idx], all_targets[idx], all_preds[idx], opti_preds[idx], class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_acc = accuracy_score(all_targets, all_preds)\n",
    "print(f\"Exact match accuracy: {subset_acc * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f189bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), fr'Models/{model.model_name}_{model.epochs_trained}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39edb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model = ViT(num_classes=14, model_name='swin_base_patch4_window7_224')\n",
    "\n",
    "checkpoint = torch.load(fr'Checkpoints/{model.model_name}_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "opti.load_state_dict(checkpoint['optimiser_state_dict'])\n",
    "model.epochs_trained = checkpoint.get('epochs_trained', 0)\n",
    "model.eval()\n",
    "model.fine_tune()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Get the Attention module in the last block\n",
    "def get_last_block_attn(model):\n",
    "    return model.model.layers[-1].blocks[-1].attn\n",
    "\n",
    "# Manually extract attn weights inside the module\n",
    "def get_attention_map(model, image_tensor):\n",
    "    attention_maps = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        # Manually recompute attn weights from inputs inside the Attention module\n",
    "        qkv = module.qkv(input[0])  # [B*nW, N, 3*C]\n",
    "        B, N, _ = qkv.shape\n",
    "        C = _ // 3\n",
    "        q, k, v = qkv.chunk(3, dim=2)\n",
    "\n",
    "        q = q.reshape(B, N, module.num_heads, C // module.num_heads).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(B, N, module.num_heads, C // module.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * module.scale\n",
    "        attn = attn.softmax(dim=-1)  # [B*nW, num_heads, N, N]\n",
    "        attention_maps.append(attn.detach())\n",
    "\n",
    "    attn_module = get_last_block_attn(model)\n",
    "    handle = attn_module.register_forward_hook(hook_fn)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(image_tensor)\n",
    "\n",
    "    handle.remove()\n",
    "    return attention_maps[0]  # [B*nW, num_heads, N, N]\n",
    "\n",
    "# Visualize the overlay of attention on the image\n",
    "def visualize_attention_on_image(img_tensor, attn_weights, head=None, mean=None, std=None):\n",
    "    img = img_tensor.squeeze(0).cpu().clone()\n",
    "\n",
    "    if mean is not None and std is not None:\n",
    "        mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        std = torch.tensor(std).view(-1, 1, 1)\n",
    "        img = img * std + mean\n",
    "\n",
    "    pil_img = to_pil_image(img)\n",
    "\n",
    "    # Extract attention\n",
    "    nW_B, nH, N, _ = attn_weights.shape\n",
    "    token_idx = N // 2  # use center token\n",
    "    if head is not None:\n",
    "        attn = attn_weights[:, head, token_idx, :]  # [nW*B, N]\n",
    "    else:\n",
    "        attn = attn_weights[:, :, token_idx, :]     # [nW*B, nH, N]\n",
    "        attn = attn.mean(1)  # mean over heads\n",
    "\n",
    "    attn = attn.mean(0)  # mean over windows\n",
    "    attn = attn.view(int(N**0.5), int(N**0.5))  # reshape\n",
    "\n",
    "    # Upsample\n",
    "    attn = attn.unsqueeze(0).unsqueeze(0)\n",
    "    attn = F.interpolate(attn, size=(img.shape[1], img.shape[2]), mode='bilinear', align_corners=False)\n",
    "    attn = attn.squeeze().cpu().numpy()\n",
    "    attn = (attn - attn.min()) / (attn.max() - attn.min() + 1e-6)\n",
    "\n",
    "    plt.imshow(pil_img)\n",
    "    plt.imshow(attn, cmap='jet', alpha=0.5)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Run on one batch\n",
    "def run_attention_overlay(model, dataloader, mean, std, head=None):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    for batch in dataloader:\n",
    "        img_tensor = batch['images'][0].unsqueeze(0).to(device)\n",
    "        print(batch['labels'][0])\n",
    "        attn = get_attention_map(model, img_tensor)\n",
    "        visualize_attention_on_image(img_tensor, attn, head=head, mean=mean, std=std)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "run_attention_overlay(model, test_dl, mean, std, head=None)  # head=None for average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e241a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CT6007",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
