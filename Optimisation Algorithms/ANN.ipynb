{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b46e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Import numpy library for numerical computations\n",
    "import pandas as pd  # Import pandas library for data manipulation and analysis\n",
    "from pyDOE import lhs  # Import lhs function from pyDOE module for Latin Hypercube Sampling\n",
    "import matplotlib.pyplot as plt # Import matplotlib to show image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853a9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('MNIST.csv')  # Read the data from the 'MNIST.csv' file into a pandas DataFrame\n",
    "data = np.array(data)  # Convert the DataFrame to a numpy array\n",
    "np.random.shuffle(data)  # Shuffle the rows of the data array randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f95c18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = data.shape  # Get the dimensions of the data array\n",
    "\n",
    "# Extract the first 1000 rows for testing data and transpose them\n",
    "data_test = data[0:1000].T\n",
    "Y_test = data_test[0]  # Extract the labels for testing data\n",
    "X_test = data_test[1:n]  # Extract the features for testing data\n",
    "X_test = X_test / 255  # Normalise the features by dividing by 255 (maximum pixel value)\n",
    "\n",
    "# Extract the remaining rows for training data and transpose them\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]  # Extract the labels for training data\n",
    "X_train = data_train[1:n]  # Extract the features for training data\n",
    "X_train = X_train / 255  # Normalise the features by dividing by 255 (maximum pixel value)\n",
    "\n",
    "_, m_train = X_train.shape  # Get the number of training examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "025dbe58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 41000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape  # Check the shape of the training data array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d717afea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, Y_train.size)\n",
    "curr_img = X_train[:, idx, None]\n",
    "curr_img = curr_img.reshape((28, 28)) * 255\n",
    "plt.gray()\n",
    "plt.imshow(curr_img)\n",
    "plt.show()\n",
    "print(Y_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9031288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self, input_size, hidden_1_size, hidden_2_size, output_size, learning_rate, data_num, bias = 1):\n",
    "        \"\"\"\n",
    "        initialise the neural network with given parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - input_size: Number of input features.\n",
    "        - hidden_1_size: Number of neurons in the first hidden layer.\n",
    "        - hidden_2_size: Number of neurons in the second hidden layer.\n",
    "        - output_size: Number of output classes.\n",
    "        - learning_rate: Learning rate for gradient descent.\n",
    "        - data_num: Number of data points in the dataset.\n",
    "        - bias: Bias term (default is 1).\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.m = data_num\n",
    "\n",
    "        # Xavier initialisation for weights\n",
    "        self.W1 = np.random.randn(hidden_1_size, input_size) * np.sqrt(1.0 / input_size)\n",
    "        self.b1 = np.zeros((hidden_1_size, bias))\n",
    "\n",
    "        self.W2 = np.random.randn(hidden_2_size, hidden_1_size) * np.sqrt(1.0 / hidden_1_size)\n",
    "        self.b2 = np.zeros((hidden_2_size, bias))\n",
    "\n",
    "        self.W3 = np.random.randn(output_size, hidden_2_size) * np.sqrt(1.0 / hidden_2_size)\n",
    "        self.b3 = np.zeros((output_size, bias))\n",
    "\n",
    "    def set_learning_rate(self, new_LR):\n",
    "        \"\"\"\n",
    "        Update the learning rate.\n",
    "\n",
    "        Parameters:\n",
    "        - new_LR: New learning rate value.\n",
    "        \"\"\"\n",
    "        self.learning_rate = new_LR\n",
    "\n",
    "    def forward_prop(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward propagation through the neural network.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data matrix.\n",
    "        \"\"\"\n",
    "        self.Z1 = self.W1.dot(X) + self.b1\n",
    "        self.A1 = ReLU(self.Z1)\n",
    "\n",
    "        self.Z2 = self.W2.dot(self.A1) + self.b2\n",
    "        self.A2 = ReLU(self.Z2)\n",
    "\n",
    "        self.Z3 = self.W3.dot(self.A2) + self.b3\n",
    "        self.A3 = softmax(self.Z3)\n",
    "\n",
    "\n",
    "    def backward_prop(self, X, y):\n",
    "        \"\"\"\n",
    "        Perform backward propagation to compute gradients.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data matrix.\n",
    "        - y: True labels.\n",
    "        \"\"\"\n",
    "        one_hot_Y = one_hot(y)\n",
    "\n",
    "        self.dZ3 = self.A3 - one_hot_Y\n",
    "        self.dW3 = 1 / self.m * self.dZ3.dot(self.A2.T)\n",
    "        self.db3 = 1 / self.m * np.sum(self.dZ3, axis=1, keepdims=True)\n",
    "\n",
    "        self.dZ2 = self.W3.T.dot(self.dZ3) * ReLU_prime(self.Z2)\n",
    "        self.dW2 = 1 / self.m * self.dZ2.dot(self.A1.T)\n",
    "        self.db2 = 1 / self.m * np.sum(self.dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        self.dZ1 = self.W2.T.dot(self.dZ2) * ReLU_prime(self.Z1)\n",
    "        self.dW1 = 1 / self.m * self.dZ1.dot(X.T)\n",
    "        self.db1 = 1 / self.m * np.sum(self.dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    def gradient_descent(self):\n",
    "        \"\"\"\n",
    "        Update weights using gradient descent.\n",
    "        \"\"\"\n",
    "        self.W1 = self.W1 - self.dW1 * self.learning_rate\n",
    "        self.b1 = self.b1 - self.db1 * self.learning_rate\n",
    "\n",
    "        self.W2 = self.W2 - self.dW2 * self.learning_rate\n",
    "        self.b2 = self.b2 - self.db2 * self.learning_rate\n",
    "\n",
    "        self.W3 = self.W3 - self.dW3 * self.learning_rate\n",
    "        self.b3 = self.b3 - self.db3 * self.learning_rate\n",
    "\n",
    "    def train_GD(self, epochs):\n",
    "        \"\"\"\n",
    "        Train the neural network using batch gradient descent.\n",
    "\n",
    "        Parameters:\n",
    "        - epochs: Number of training epochs.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            self.forward_prop(X_train)\n",
    "            self.backward_prop(X_train, Y_train)\n",
    "            self.gradient_descent()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)        \n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "\n",
    "\n",
    "    def train_SGD(self, epochs, batch = 64):\n",
    "        num_samples = X_train.shape[1]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indicies = np.random.permutation(num_samples)\n",
    "            X_train_shuffled = X_train[:, indicies]\n",
    "            Y_train_shuffled = Y_train[indicies]\n",
    "\n",
    "            for i in range(0, num_samples, batch):\n",
    "                X_batch = X_train_shuffled[:, i:i+batch]\n",
    "                Y_batch = Y_train_shuffled[i:i+batch]\n",
    "\n",
    "                self.forward_prop(X_batch)\n",
    "                self.backward_prop(X_batch, Y_batch)\n",
    "                self.gradient_descent()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)        \n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "    def train_adam(self, epochs):\n",
    "        \"\"\"\n",
    "        Train the neural network using the Adam optimization algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - epochs: Number of training epochs.\n",
    "        \"\"\"        \n",
    "        #Init Extra Weights\n",
    "        self.beta1 = 0.6 # Exp decay rate for mean of gradients\n",
    "        self.beta2 = 0.6 # Exp decay rate for varience of gradients\n",
    "        self.epsilon = 1e-8 # Prevent divisions by 0\n",
    "        \n",
    "        self.m_W1 = np.zeros_like(self.W1) # Moving average of gradients\n",
    "        self.v_W1 = np.zeros_like(self.W1) # Squared moving averages of gradients\n",
    "        self.m_b1 = np.zeros_like(self.b1)\n",
    "        self.v_b1 = np.zeros_like(self.b1)\n",
    "        self.m_W2 = np.zeros_like(self.W2)\n",
    "        self.v_W2 = np.zeros_like(self.W2)\n",
    "        self.m_b2 = np.zeros_like(self.b2)\n",
    "        self.v_b2 = np.zeros_like(self.b2)\n",
    "        self.m_W3 = np.zeros_like(self.W3)\n",
    "        self.v_W3 = np.zeros_like(self.W3)\n",
    "        self.m_b3 = np.zeros_like(self.b3)\n",
    "        self.v_b3 = np.zeros_like(self.b3)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Perform forward and backward propagation\n",
    "            self.forward_prop(X_train)\n",
    "            self.backward_prop(X_train, Y_train)\n",
    "\n",
    "            # Update parameters using Adam optimization rule\n",
    "            self.adam_update()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                # Evaluate and print accuracy every 10 epochs\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)\n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        # Evaluate accuracy on test set after training\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "\n",
    "    def adam_update(self):\n",
    "        \"\"\"\n",
    "        Update weights and biases using the Adam optimization algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # Update the first moment estimate of gradients for weights W1 using exponential decay\n",
    "        self.m_W1 = self.beta1 * self.m_W1 + (1 - self.beta1) * self.dW1\n",
    "\n",
    "        # Update the second moment estimate of gradients for weights W1 using exponential decay\n",
    "        self.v_W1 = self.beta2 * self.v_W1 + (1 - self.beta2) * (self.dW1 ** 2)\n",
    "\n",
    "        # Correct bias in the first moment estimate\n",
    "        m_W1_hat = self.m_W1 / (1 - self.beta1)\n",
    "\n",
    "        # Correct bias in the second moment estimate\n",
    "        v_W1_hat = self.v_W1 / (1 - self.beta2)\n",
    "\n",
    "        # Update weights W1 using the Adam optimisation update rule\n",
    "        self.W1 -= self.learning_rate * m_W1_hat / (np.sqrt(v_W1_hat) + self.epsilon)\n",
    "\n",
    "        #Repeat for following weights and biases\n",
    "\n",
    "        self.m_b1 = self.beta1 * self.m_b1 + (1 - self.beta1) * self.db1\n",
    "        self.v_b1 = self.beta2 * self.v_b1 + (1 - self.beta2) * (self.db1 ** 2)\n",
    "        m_b1_hat = self.m_b1 / (1 - self.beta1)\n",
    "        v_b1_hat = self.v_b1 / (1 - self.beta2)\n",
    "        self.b1 -= self.learning_rate * m_b1_hat / (np.sqrt(v_b1_hat) + self.epsilon)\n",
    "\n",
    "        self.m_W2 = self.beta1 * self.m_W2 + (1 - self.beta1) * self.dW2\n",
    "        self.v_W2 = self.beta2 * self.v_W2 + (1 - self.beta2) * (self.dW2 ** 2)\n",
    "        m_W2_hat = self.m_W2 / (1 - self.beta1)\n",
    "        v_W2_hat = self.v_W2 / (1 - self.beta2)\n",
    "        self.W2 -= self.learning_rate * m_W2_hat / (np.sqrt(v_W2_hat) + self.epsilon)\n",
    "\n",
    "        self.m_b2 = self.beta1 * self.m_b2 + (1 - self.beta1) * self.db2\n",
    "        self.v_b2 = self.beta2 * self.v_b2 + (1 - self.beta2) * (self.db2 ** 2)\n",
    "        m_b2_hat = self.m_b2 / (1 - self.beta1)\n",
    "        v_b2_hat = self.v_b2 / (1 - self.beta2)\n",
    "        self.b2 -= self.learning_rate * m_b2_hat / (np.sqrt(v_b2_hat) + self.epsilon)\n",
    "\n",
    "        self.m_W3 = self.beta1 * self.m_W3 + (1 - self.beta1) * self.dW3\n",
    "        self.v_W3 = self.beta2 * self.v_W3 + (1 - self.beta2) * (self.dW3 ** 2)\n",
    "        m_W3_hat = self.m_W3 / (1 - self.beta1)\n",
    "        v_W3_hat = self.v_W3 / (1 - self.beta2)\n",
    "        self.W3 -= self.learning_rate * m_W3_hat / (np.sqrt(v_W3_hat) + self.epsilon)\n",
    "\n",
    "        self.m_b3 = self.beta1 * self.m_b3 + (1 - self.beta1) * self.db3\n",
    "        self.v_b3 = self.beta2 * self.v_b3 + (1 - self.beta2) * (self.db3 ** 2)\n",
    "        m_b3_hat = self.m_b3 / (1 - self.beta1)\n",
    "        v_b3_hat = self.v_b3 / (1 - self.beta2)\n",
    "        self.b3 -= self.learning_rate * m_b3_hat / (np.sqrt(v_b3_hat) + self.epsilon)\n",
    "\n",
    "    \n",
    "   \n",
    "    def train_pso(self, epochs, n_particles=50, w_start=0.9, w_end=0.3, c1=0.5, c2=0.3):\n",
    "        \"\"\"\n",
    "        Train the neural network using Particle Swarm Optimization (PSO).\n",
    "\n",
    "        Parameters:\n",
    "        - epochs: Number of training epochs.\n",
    "        - n_particles: Number of particles (default is 50).\n",
    "        - w_start: Initial inertia weight (default is 0.9).\n",
    "        - w_end: Final inertia weight (default is 0.2).\n",
    "        - c1: Cognitive parameter (default is 1.5).\n",
    "        - c2: Social parameter (default is 1.5).\n",
    "        \"\"\"\n",
    "        # initialise particles' positions using Latin Hypercube Sampling\n",
    "        particle_positions = self._latin_hypercube_sampling(n_particles)\n",
    "        # initialise particles' velocities\n",
    "        particle_velocities = [np.random.randn(len(particle_positions[0])) * 0.1 for _ in range(n_particles)]\n",
    "        # initialise personal best positions and scores\n",
    "        personal_best_positions = particle_positions.copy()\n",
    "        personal_best_scores = [self._evaluate_fitness(p) for p in personal_best_positions]\n",
    "        # initialise global best position\n",
    "        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # Update inertia weight dynamically\n",
    "            w = w_start - (w_start - w_end) * epoch / epochs\n",
    "\n",
    "            for i in range(n_particles):\n",
    "                # Update velocity\n",
    "                r1, r2 = np.random.rand(), np.random.rand()\n",
    "                particle_velocities[i] = (\n",
    "                    w * particle_velocities[i]\n",
    "                    + c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n",
    "                    + c2 * r2 * (global_best_position - particle_positions[i])\n",
    "                )\n",
    "                \n",
    "                # Update position\n",
    "                particle_positions[i] += particle_velocities[i]\n",
    "                # Evaluate fitness\n",
    "                score = self._evaluate_fitness(particle_positions[i])\n",
    "                if score < self._evaluate_fitness(personal_best_positions[i]):\n",
    "                    personal_best_positions[i] = particle_positions[i].copy()\n",
    "                    if score < self._evaluate_fitness(global_best_position):\n",
    "                        global_best_position = particle_positions[i].copy()\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                # Set weights from the global best position and evaluate accuracy\n",
    "                self._set_weights_from_vector(global_best_position)\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)\n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        # Set final weights from the global best position and evaluate accuracy on test set\n",
    "        self._set_weights_from_vector(global_best_position)\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "\n",
    "    def _latin_hypercube_sampling(self, n_particles):\n",
    "        \"\"\"\n",
    "        Generate Latin Hypercube Samples for particle positions.\n",
    "\n",
    "        Parameters:\n",
    "        - n_particles: Number of particles.\n",
    "\n",
    "        Returns:\n",
    "        - Scaled Latin Hypercube Samples for particle positions.\n",
    "        \"\"\"\n",
    "        samples = lhs(self._get_weights_as_vector().size, samples=n_particles)\n",
    "        min_val = -1.0\n",
    "        max_val = 1.0\n",
    "        scaled_samples = min_val + samples * (max_val - min_val)\n",
    "        return scaled_samples\n",
    "\n",
    "    def _get_weights_as_vector(self):\n",
    "        \"\"\"\n",
    "        Concatenate weights and biases into a single vector.\n",
    "\n",
    "        Returns:\n",
    "        - Flattened vector containing weights and biases.\n",
    "        \"\"\"\n",
    "        return np.concatenate([\n",
    "            self.W1.flatten(), self.b1.flatten(),\n",
    "            self.W2.flatten(), self.b2.flatten(),\n",
    "            self.W3.flatten(), self.b3.flatten()\n",
    "        ])\n",
    "\n",
    "    def _set_weights_from_vector(self, vector):\n",
    "        \"\"\"\n",
    "        Set weights and biases from a vector.\n",
    "\n",
    "        Parameters:\n",
    "        - vector: Vector containing weights and biases.\n",
    "        \"\"\"\n",
    "        sizes = [\n",
    "            self.W1.size, self.b1.size,\n",
    "            self.W2.size, self.b2.size,\n",
    "            self.W3.size, self.b3.size\n",
    "        ]\n",
    "        vectors = np.split(vector, np.cumsum(sizes)[:-1])\n",
    "        self.W1 = vectors[0].reshape(self.W1.shape)\n",
    "        self.b1 = vectors[1].reshape(self.b1.shape)\n",
    "        self.W2 = vectors[2].reshape(self.W2.shape)\n",
    "        self.b2 = vectors[3].reshape(self.b2.shape)\n",
    "        self.W3 = vectors[4].reshape(self.W3.shape)\n",
    "        self.b3 = vectors[5].reshape(self.b3.shape)\n",
    "\n",
    "    def _evaluate_fitness(self, weights_vector):\n",
    "        \"\"\"\n",
    "        Evaluate fitness using weights vector.\n",
    "\n",
    "        Parameters:\n",
    "        - weights_vector: Vector containing weights.\n",
    "\n",
    "        Returns:\n",
    "        - Negative accuracy for minimization.\n",
    "        \"\"\"\n",
    "        self._set_weights_from_vector(weights_vector)\n",
    "        self.forward_prop(X_train)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        return -np.sum(predictions == Y_train) / Y_train.size  # Negative accuracy for minimization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, method, epochs):\n",
    "        \"\"\"\n",
    "        Train the neural network using the specified method.\n",
    "\n",
    "        Parameters:\n",
    "        - method: String indicating the training method ('GD', 'SGD', 'adam', 'pso').\n",
    "        - epochs: Number of training epochs.\n",
    "        \"\"\"\n",
    "        if method == 'GD':\n",
    "            self.train_GD(epochs)\n",
    "        elif method == 'SGD':\n",
    "            self.train_SGD(epochs)\n",
    "        elif method == 'adam':\n",
    "            self.train_adam(epochs)\n",
    "        elif method == 'pso':\n",
    "            self.train_pso(epochs)\n",
    "        else:\n",
    "            raise ValueError('Invalid Training Method')\n",
    "\n",
    "def ReLU(Z):\n",
    "    \"\"\"\n",
    "    ReLU activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the ReLU function.\n",
    "\n",
    "    Returns:\n",
    "    - Output of the ReLU function.\n",
    "    \"\"\"\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_prime(Z):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the ReLU function.\n",
    "\n",
    "    Returns:\n",
    "    - Derivative of the ReLU function.\n",
    "    \"\"\"\n",
    "    return Z > 0\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "    - Output of the sigmoid function.\n",
    "    \"\"\"\n",
    "    A = 1 / (1 + np.exp(np.clip(-Z, -4, 4)))\n",
    "    return A\n",
    "\n",
    "def sigmoid_prime(Z):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "    - Derivative of the sigmoid function.\n",
    "    \"\"\"\n",
    "    A = sigmoid(Z) * (1 - sigmoid(Z))\n",
    "    return A\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Softmax activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the softmax function.\n",
    "\n",
    "    Returns:\n",
    "    - Output of the softmax function.\n",
    "    \"\"\"\n",
    "    Z_shifted = Z - np.max(Z, axis=0, keepdims=True)  # Shift values for numerical stability\n",
    "    exp_Z = np.exp(Z_shifted)\n",
    "    A = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    return A\n",
    "\n",
    "def one_hot(Y):\n",
    "    \"\"\"\n",
    "    Convert class labels to one-hot encoding.\n",
    "\n",
    "    Parameters:\n",
    "    - Y: Class labels.\n",
    "\n",
    "    Returns:\n",
    "    - One-hot encoded representation of class labels.\n",
    "    \"\"\"\n",
    "    one_hot_Y = np.zeros((Y.size, 10))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b208f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Neural_Network(784, 32, 16, 10, 0.01, m_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3950c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 24.461%\n",
      "Epoch 10: 70.6537%\n",
      "Epoch 20: 82.2073%\n",
      "Epoch 30: 89.3951%\n",
      "Epoch 40: 88.0317%\n",
      "Epoch 50: 88.2463%\n",
      "Epoch 60: 92.978%\n",
      "Epoch 70: 88.9878%\n",
      "Epoch 80: 92.8098%\n",
      "Epoch 90: 94.2878%\n",
      "Epoch 100: 94.0439%\n",
      "Epoch 110: 95.4244%\n",
      "Epoch 120: 95.8976%\n",
      "Epoch 130: 95.4415%\n",
      "Epoch 140: 95.7585%\n",
      "Epoch 150: 95.2951%\n",
      "Epoch 160: 95.0732%\n",
      "Epoch 170: 96.0902%\n",
      "Epoch 180: 95.6829%\n",
      "Epoch 190: 95.1195%\n",
      "Training complete\n",
      "\n",
      "Test Set Accuracy: 93.2%\n"
     ]
    }
   ],
   "source": [
    "nn.set_learning_rate(0.05)\n",
    "nn.train('adam', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d2bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
